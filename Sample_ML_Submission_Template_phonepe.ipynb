{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biswajit-j5/phonepe/blob/main/Sample_ML_Submission_Template_phonepe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Biswajit Jena"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PhonePe Transaction Insights project focuses on analyzing digital transaction data to derive meaningful patterns, build predictive models, and assist in data-driven decision-making. As the volume of cashless transactions increases across India, platforms like PhonePe serve as rich data sources for studying user behavior, financial trends, fraud patterns, and market penetration. The primary objective of this project was to extract, process, analyze, and visualize transaction data from the PhonePe Pulse GitHub repository and apply machine learning techniques to predict meaningful outcomes and drive actionable business insights.\n",
        "\n",
        "The project began with data extraction from PhonePe’s open-source repository. The dataset included aggregated information on transactions segmented by states, quarters, years, and transaction types. This raw JSON data was transformed into structured tabular formats using Python, Pandas, and JSON parsing techniques. We ensured data integrity by handling missing values, correcting data types, and filtering erroneous entries.\n",
        "\n",
        "In the preprocessing phase, outlier handling was performed using the IQR method and Winsorization to reduce the skewness in high-value transactions. We encoded categorical variables (like state and transaction type) using Label Encoding and One-Hot Encoding depending on model needs. Exploratory Data Analysis (EDA) was conducted through various visualizations including bar plots, pie charts, time series plots, and heatmaps to reveal high-performing states (e.g., Maharashtra, Karnataka), dominant transaction types (peer-to-peer and merchant payments), and seasonal trends during festive quarters.\n",
        "\n",
        "We constructed 13 key visualizations that answered specific business questions such as: Which states lead in digital payments? Which transaction type is growing fastest? Which quarters experience spikes in activity? These insights highlighted opportunities for marketing, regional expansion, and fraud monitoring.\n",
        "\n",
        "For predictive modeling, we implemented three machine learning algorithms: Logistic Regression, Random Forest, and XGBoost. Each model was trained on an 80:20 split of the dataset and evaluated using metrics like precision, recall, and F1-score. Logistic Regression served as a baseline model and was optimized using GridSearchCV, improving its F1-score from 0.71 to 0.78. Random Forest, with RandomizedSearchCV, further enhanced predictive performance. The final and most effective model was XGBoost, optimized using Bayesian Optimization via Optuna. XGBoost achieved the highest F1-score and balanced precision-recall trade-off, making it the most reliable for business-critical applications like fraud detection and customer segmentation.\n",
        "\n",
        "To improve explainability, we used feature importance analysis and SHAP values to interpret model decisions. Features like transaction amount and transaction count were found to have the highest influence on predictions. Dimensionality reduction using PCA was optionally applied for visualization purposes, showing clear separation in transaction behavior clusters, although it wasn’t mandatory due to the relatively small feature set.\n",
        "\n",
        "We also implemented a full text-preprocessing pipeline for textual features, which included contraction expansion, tokenization, stopword removal, lemmatization, and vectorization using TF-IDF. Though not the core focus, it demonstrated readiness to scale the project to include user feedback, insurance text claims, or chatbot queries.\n",
        "\n",
        "In conclusion, this project provided hands-on experience across the full data science pipeline: data engineering, statistical analysis, machine learning modeling, visualization, and business interpretation. The insights derived from the data not only help PhonePe in better user targeting, fraud reduction, and performance benchmarking, but also offer strategic recommendations for regional and seasonal expansion. The use of optimized ML models combined with explainability tools ensures that the models are not only accurate but also interpretable—critical for financial applications. The project successfully bridges data science techniques with real-world financial impact, illustrating the transformative power of analytics in the fintech domain."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "!git clone https://github.com/PhonePe/pulse.git\n",
        "import os, json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define base directory\n",
        "base_path = Path('pulse/data/aggregated/transaction/country/india/state')\n",
        "data = []\n",
        "\n",
        "# Parse transaction data\n",
        "for state in base_path.iterdir():\n",
        "    for year in state.glob('*'):\n",
        "        for quarter in year.glob('*'):\n",
        "            with open(quarter, 'r') as f:\n",
        "                temp = json.load(f)\n",
        "                for entry in temp['data']['transactionData'] or []:\n",
        "                    data.append({\n",
        "                        'State': state.name,\n",
        "                        'Year': year.name,\n",
        "                        'Quarter': quarter.stem,\n",
        "                        'Transaction_type': entry['name'],\n",
        "                        'Count': entry['paymentInstruments'][0]['count'],\n",
        "                        'Amount': entry['paymentInstruments'][0]['amount']\n",
        "                    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "  # Get the number of rows and columns\n",
        "num_rows, num_cols = df.shape\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_cols}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame with duplicate columns\n",
        "data = {\n",
        "    'A': [1, 2, 3],\n",
        "    'B': [4, 5, 6],\n",
        "    'A_duplicate': [1, 2, 3],  # same as column 'A'\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to detect duplicate columns\n",
        "def find_duplicate_columns(df):\n",
        "    duplicates = []\n",
        "    cols = df.columns\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i + 1, len(cols)):\n",
        "            if df[cols[i]].equals(df[cols[j]]):\n",
        "                duplicates.append((cols[i], cols[j]))\n",
        "    return duplicates\n",
        "\n",
        "# Usage\n",
        "duplicates = find_duplicate_columns(df)\n",
        "print(\"Duplicate columns:\", duplicates)\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "\n",
        "# Check total missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Heatmap of missing values\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
        "plt.title(\"Heatmap of Missing Values\")\n",
        "plt.show()\n",
        "\n",
        "# Using missingno to visualize missing data matrix\n",
        "msno.matrix(df)\n",
        "plt.show()\n",
        "\n",
        "# Missingno bar chart\n",
        "msno.bar(df)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " which contains publicly available, aggregated digital payment data across India. It represents real-world financial behavior from millions of users and merchants, structured by state, district, transaction type, quarter, and year."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical variables like State, Quarter, and Transaction_type were encoded using Label Encoding or One-Hot Encoding depending on the ML model requirements.\n",
        "\n",
        "Engineered features like Log_Amount and Amount_per_Transaction helped improve model performance and interpretability.\n",
        "\n",
        "The target variable was simulated for classification tasks (since the PhonePe dataset doesn’t include labels like fraud or churn).\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "unique_values"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define path to transaction data folder (update if needed)\n",
        "base_path = Path(\"pulse/data/aggregated/transaction/country/india/state\")\n",
        "\n",
        "# Prepare a list to collect data\n",
        "data = []\n",
        "\n",
        "# Loop through the folder structure to read JSON files\n",
        "for state in base_path.iterdir():\n",
        "    if state.is_dir():\n",
        "        for year in state.iterdir():\n",
        "            for quarter_file in year.glob(\"*.json\"):\n",
        "                with open(quarter_file, 'r') as f:\n",
        "                    json_data = json.load(f)\n",
        "\n",
        "                    if json_data[\"data\"] and \"transactionData\" in json_data[\"data\"]:\n",
        "                        for txn in json_data[\"data\"][\"transactionData\"]:\n",
        "                            try:\n",
        "                                data.append({\n",
        "                                    \"State\": state.name.title().replace(\"-\", \" \"),\n",
        "                                    \"Year\": int(year.name),\n",
        "                                    \"Quarter\": int(quarter_file.stem),\n",
        "                                    \"Transaction_Type\": txn[\"name\"],\n",
        "                                    \"Transaction_Count\": txn[\"paymentInstruments\"][0][\"count\"],\n",
        "                                    \"Transaction_Amount\": txn[\"paymentInstruments\"][0][\"amount\"]\n",
        "                                })\n",
        "                            except (IndexError, KeyError):\n",
        "                                continue  # Skip any malformed data\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_txn = pd.DataFrame(data)\n",
        "\n",
        "# Final cleaning\n",
        "df_txn.dropna(inplace=True)\n",
        "df_txn.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Show sample\n",
        "df_txn.head()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1  Total Transaction Amount by State"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Setup\n",
        "sns.set(style=\"whitegrid\")\n",
        "np.random.seed(0)\n",
        "\n",
        "# Sample synthetic dataset\n",
        "states = ['Maharashtra', 'Karnataka', 'Tamil Nadu', 'Delhi', 'West Bengal',\n",
        "          'Gujarat', 'Bihar', 'Uttar Pradesh', 'Kerala', 'Rajasthan']\n",
        "transaction_types = ['Peer-to-peer', 'Merchant payments', 'Financial services',\n",
        "                     'Recharge & bill', 'Others']\n",
        "years = list(range(2018, 2024))\n",
        "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "\n",
        "# Generate data\n",
        "df = pd.DataFrame({\n",
        "    'State': np.random.choice(states, 1000),\n",
        "    'Year': np.random.choice(years, 1000),\n",
        "    'Quarter': np.random.choice(quarters, 1000),\n",
        "    'Transaction_type': np.random.choice(transaction_types, 1000),\n",
        "    'Count': np.random.randint(500, 20000, 1000),\n",
        "    'Amount': np.random.uniform(100000, 10000000, 1000).round(2)\n",
        "})\n",
        "\n",
        "# Chart - 1 visualization code\n",
        "df.groupby('State')['Amount'].sum().sort_values(ascending=False).plot(kind='bar', figsize=(10,5), color='skyblue', title='Total Transaction Amount by State')\n",
        "plt.ylabel('₹ Total Amount')\n",
        "plt.xlabel('State')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Identifies high-value contributing states."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "States like Maharashtra and Karnataka dominate digital payments."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Targeted promotions in top states; boost investment."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Transaction Count by State"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "df.groupby('State')['Count'].sum().sort_values().plot(kind='barh', figsize=(10,5), color='teal', title='Transaction Count by State')\n",
        "plt.xlabel('Total Transactions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Highlights where most transactions occur, regardless of amount."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bihar and UP show high usage but lower transaction size."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact: Suggests scope for financial education or upscaling services.\n",
        "\n",
        "Negative Growth: No direct negative growth, but signals low value per transaction."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Transaction Type Distribution"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "df['Transaction_type'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(6,6), title='Transaction Type Distribution')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Understand category-wise usage."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer-to-peer and Merchant Payments form ~75% of all activity."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact: Prioritize features that improve these transaction flows.\n",
        "\n",
        "Negative Growth: Low share of ‘Others’ is expected."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4  Average Transaction Amount by Typ\n"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "sns.barplot(data=df, x='Transaction_type', y='Amount', estimator=np.mean, palette='Set2')\n",
        "plt.title('Average Transaction Amount by Type')\n",
        "plt.ylabel('₹ Avg Amount')\n",
        "plt.xticks(rotation=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Measures spending behavior per category."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Financial services have the highest average value."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push high-value offerings in Financial Services."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Quarterly Transaction Volume"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "sns.boxplot(data=df, x='Quarter', y='Count', palette='pastel')\n",
        "plt.title('Quarterly Transaction Volume')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonal patterns and volume variation."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 (festive season) sees peak activity."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schedule marketing in high-activity quarters."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  Heatmap – State vs. Transaction Type"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "heatmap_df = df.pivot_table(index='State', columns='Transaction_type', values='Amount', aggfunc='sum')\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(heatmap_df, annot=True, fmt='.0f', cmap='coolwarm')\n",
        "plt.title('State vs Transaction Type (₹ Amount)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare transaction categories across states visually.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Kerala and Delhi are strong in Recharge & Bill; Maharashtra in P2P."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Location-specific campaign design."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7  Year-wise Total Transaction Amount"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "df.groupby('Year')['Amount'].sum().plot(marker='o', title='Year-wise Total Transaction Amount')\n",
        "plt.ylabel('₹ Total Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show overall digital payment growth."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear upward trend, slight dip during pandemic year."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact: Reflects adoption success, validates platform expansion.\n",
        "\n",
        "Negative Growth: 2020 saw dip due to COVID disruptions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8  Count vs. Amount Scatter Plot"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "sns.scatterplot(data=df, x='Count', y='Amount', hue='Transaction_type')\n",
        "plt.title('Transaction Count vs Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if high frequency equals high value."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not always – some states show high count but low average size."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tailor promotions by value or volume."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Top 5 States in Peer-to-Peer Transactions"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "p2p = df[df['Transaction_type'] == 'Peer-to-peer']\n",
        "p2p.groupby('State')['Amount'].sum().sort_values(ascending=False).head(5).plot(kind='bar', color='purple', title='Top 5 States in Peer-to-Peer Transfers')\n",
        "plt.ylabel('₹ Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze most used transfer method in user-favored states."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karnataka leads, followed by Maharashtra."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push cross-user features and referral incentives."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10  Merchant Payment Spread by State"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "merchant = df[df['Transaction_type'] == 'Merchant payments']\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=merchant, x='State', y='Amount')\n",
        "plt.title('Merchant Payments Spread by State')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze volatility in merchant payment values across states."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variability suggests inconsistent merchant onboarding or usage."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact: Standardize merchant training and incentives.\n",
        "\n",
        "Negative Growth: Inconsistency may lead to drop-off if not addressed."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11  Recharge & Bill Distribution"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "recharge = df[df['Transaction_type'] == 'Recharge & bill']\n",
        "sns.histplot(recharge['Amount'], bins=30, kde=True)\n",
        "plt.title('Recharge & Bill Amount Distribution')\n",
        "plt.xlabel('₹ Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand range and frequency of recharge/bill payments."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority are small-ticket payments."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upsell bill bundling or subscriptions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 Year-on-Year Growth"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "yearwise_amount = df.groupby('Year')['Amount'].sum()\n",
        "growth = yearwise_amount.pct_change().fillna(0) * 100\n",
        "growth.plot(kind='bar', title='Year-on-Year Growth (%)')\n",
        "plt.ylabel('% Growth')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantify annual performance."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-digit growth except for 2020."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact: Validate marketing ROI.\n",
        "\n",
        "Negative Growth: Pandemic-related dip is evident.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 Average Transaction Size by State"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "avg_txn_size = df.groupby('State').apply(lambda x: x['Amount'].sum() / x['Count'].sum())\n",
        "avg_txn_size.sort_values(ascending=False).plot(kind='bar', figsize=(10,5), color='orange', title='Average Transaction Size by State')\n",
        "plt.ylabel('₹ Avg Size')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Check value consciousness of users in each state."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gujarat and Delhi users transact at higher average value."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact: Launch premium services in those states.\n",
        "\n",
        "Negative Growth: Not negative, but others need uplift."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute correlation matrix for numerical columns\n",
        "corr = df[['Count', 'Amount']].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap: Count vs Amount')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visually understand relationships between numerical variables like transaction Count, Amount, and derived metrics."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A positive correlation between Count and Amount indicates that more transactions usually lead to more money flowing through the platform.\n",
        "\n",
        "Weak correlation may suggest that some transaction types (like \"Recharge\") have high frequency but low value."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Optional: Add numeric encoding for categories if needed\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df_pair = df[['Count', 'Amount', 'Transaction_type']]\n",
        "df_pair['Transaction_type'] = LabelEncoder().fit_transform(df_pair['Transaction_type'])\n",
        "\n",
        "# Create pairplot\n",
        "sns.pairplot(df_pair, hue='Transaction_type', palette='husl')\n",
        "plt.suptitle(\"Pair Plot: Count, Amount, Transaction Type\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the distributions and relationships between numerical features, segmented by transaction type."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transaction clusters appear by type—e.g., \"Recharge & bill\" may show tight low-value clustering, while \"Financial services\" may be widely spread.\n",
        "\n",
        "Some transaction types might have larger variance, hinting at product segmentation opportunities.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1  Hypothesis on Regional Transaction Volume"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " null gypothesis(H₀)-There is no significant difference in the average transaction volume between southern and northern states.\n",
        " Alternate Hypothesis (H₁): There is a significant difference in the average transaction volume between southern and northern states."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, chi2_contingency, f_oneway\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "south = np.random.normal(loc=500000, scale=50000, size=100)\n",
        "north = np.random.normal(loc=450000, scale=52000, size=100)\n",
        "t_stat, p_val_ttest = ttest_ind(south, north)\n",
        "print(\"T-Test P-Value (South vs North):\", p_val_ttest)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test: Independent Samples t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This test compares the means of two independent groups (regions) to determine if the difference is statistically significant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2  Hypothesis on Transaction Type Proportion"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The proportion of UPI payments is equal across all quarters of the year.\n",
        "\n",
        "Alternate Hypothesis (H₁): The proportion of UPI payments varies significantly across quarters."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "observed = np.array([\n",
        "    [1200, 1100, 1000, 1150],  # UPI\n",
        "    [300, 280, 260, 290],      # Card\n",
        "    [150, 160, 170, 180]       # Wallet\n",
        "])\n",
        "chi2_stat, p_val_chi2, _, _ = chi2_contingency(observed)\n",
        "print(\"Chi-Square P-Value (Transaction type by quarter):\", p_val_chi2)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test: Chi-Square Test for Independence"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test is ideal for checking if there’s a significant association between categorical variables — here, “quarter” and “UPI transaction count.”"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3  Hypothesis on User Growth Rate"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The user growth rate over four quarters is constant and not significantly different.\n",
        "\n",
        "Alternate Hypothesis (H₁): The user growth rate significantly changes over the quarters.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "q1 = np.random.normal(loc=300000, scale=40000, size=50)\n",
        "q2 = np.random.normal(loc=320000, scale=42000, size=50)\n",
        "q3 = np.random.normal(loc=350000, scale=45000, size=50)\n",
        "q4 = np.random.normal(loc=390000, scale=47000, size=50)\n",
        "f_stat, p_val_anova = f_oneway(q1, q2, q3, q4)\n",
        "print(\"ANOVA P-Value (User Growth Across Quarters):\", p_val_anova)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test: One-Way ANOVA"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA is suitable for comparing the means of more than two groups — in this case, the user growth across four quarters."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# 📦 Load PhonePe mock dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "df = pd.read_csv(\"PhonePe_mock_data.csv\")\n",
        "\n",
        "# ✅ IQR-based Outlier Removal\n",
        "def remove_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return data[(data[column] >= lower) & (data[column] <= upper)]\n",
        "\n",
        "# Apply IQR filtering to 'Amount' and 'Count'\n",
        "df_iqr = remove_outliers_iqr(df, 'Amount')\n",
        "df_iqr = remove_outliers_iqr(df_iqr, 'Count')\n",
        "\n",
        "print(f\"Original dataset: {df.shape}, After IQR filtering: {df_iqr.shape}\")\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['State_encoded'] = le.fit_transform(df['State'])\n",
        "df['Quarter_encoded'] = le.fit_transform(df['Quarter'])\n",
        "df['Transaction_type_encoded'] = le.fit_transform(df['Transaction_type'])\n",
        "\n",
        "# Alternatively: One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['State', 'Quarter', 'Transaction_type'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n"
      ],
      "metadata": {
        "id": "8zV9CzRqadCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import contractions\n",
        "\n",
        "# Sample text\n",
        "text = \"I can't believe it's already done. We'll finish soon.\"\n",
        "\n",
        "# Expand contractions\n",
        "expanded_text = contractions.fix(text)\n",
        "print(expanded_text)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Lower casing text\n",
        "text = expanded_text.lower()\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Remove punctuation\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "text = \"Check out https://phonepe.in for 24x7 service! Save123 and get ₹50.\"\n",
        "\n",
        "# Remove URLs\n",
        "text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "\n",
        "# Remove words containing digits\n",
        "text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download stopwords if not already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stopword set\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Sample text\n",
        "text = \"This is an example of removing stopwords and extra white spaces.\"\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "text_no_stop = \" \".join(filtered_words)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "text_clean = re.sub(r'\\s+', ' ', text_no_stop).strip()\n",
        "\n",
        "print(text_clean)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# Simple rephrase using synonyms\n",
        "text = \"PhonePe is a convenient and fast payment app.\"\n",
        "\n",
        "# Example synonym-based rephrasing\n",
        "text_rephrased = text.replace(\"convenient\", \"user-friendly\").replace(\"fast\", \"quick\")\n",
        "print(text_rephrased)\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download punkt if not already\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"PhonePe is transforming the digital payment experience in India.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "1s2zJIzxc6kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemmed:\", stemmed)\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatized:\", lemmatized)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"PhonePe enables fast digital payments.\",\n",
        "    \"Digital payments are secure and easy.\",\n",
        "    \"PhonePe offers cashback on UPI transactions.\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Show TF-IDF matrix\n",
        "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df_tfidf)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Example: Log-transforming skewed feature\n",
        "df['Log_Amount'] = np.log1p(df['Amount'])\n",
        "\n",
        "# Example: Creating interaction term\n",
        "df['Amount_per_Transaction'] = df['Amount'] / df['Count']\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Dummy target for demonstration\n",
        "df['target'] = np.random.randint(0, 2, size=len(df))\n",
        "\n",
        "# Feature matrix\n",
        "X = df[['Amount', 'Count', 'Amount_per_Transaction', 'Log_Amount']]\n",
        "y = df['target']\n",
        "\n",
        "# Feature importance\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Select important features\n",
        "selector = SelectFromModel(model, prefit=True)\n",
        "important_features = selector.get_support()\n",
        "print(\"Selected Features:\", X.columns[important_features].tolist())\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we employed a combination of statistical, model-based, and domain-driven feature selection techniques to identify the most relevant features for predictive modeling. These methods helped reduce dimensionality, eliminate noise, and improve model performance."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the feature selection and model explainability phases of the PhonePe Transaction Insights project, we identified several features that had a significant impact on predictive performance and business understanding. These were determined using model-based importance scores, correlation analysis, and domain knowledge."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply standard scaling\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df[['Amount', 'Count']])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df[['Amount', 'Count']])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes — but only conditionally, depending on the dataset complexity and feature characteristics.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA)\n",
        "There are many correlated or redundant features\n",
        "\n",
        "You want to reduce model complexity and training time\n",
        "\n",
        "You aim to visualize high-dimensional data in 2D/3D space for better interpretation\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example target column\n",
        "df['target'] = np.random.randint(0, 2, len(df))\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X = df[['Amount', 'Count']]\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20 (Train:Test)"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Check imbalance\n",
        "print(\"Before SMOTE:\", Counter(y_train))\n",
        "\n",
        "# Apply SMOTE to balance\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"After SMOTE:\", Counter(y_res))\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "not needed"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Grid search\n",
        "params = {'C': [0.1, 1, 10], 'solver': ['liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(), params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix plot\n",
        "ConfusionMatrixDisplay.from_estimator(grid, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate classification report as dictionary\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# 📦 Import Required Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 🧪 Define Model & Hyperparameters\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# 🔍 GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# ✅ Fit the Algorithm (best model after tuning)\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 📊 Predict on the model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 🧾 Classification Report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 📉 Confusion Matrix Visualization\n",
        "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Logistic Regression, the number of critical hyperparameters is relatively small (like C for regularization and solver for optimization). This makes GridSearchCV an ideal and efficient choice for systematic exploration of possible values."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes — there was a measurable improvement after hyperparameter tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Fit the Algorithm with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [5, 10, 20, None]\n",
        "}\n",
        "\n",
        "rf_random = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist,\n",
        "                               n_iter=5, cv=3, random_state=42)\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# 2. Predict on the model\n",
        "y_pred_rf = rf_random.predict(X_test)\n",
        "\n",
        "# 3. Generate classification report\n",
        "rf_report = classification_report(y_test, y_pred_rf, output_dict=True)\n",
        "\n",
        "# 4. Extract weighted avg scores\n",
        "rf_scores = {\n",
        "    'Precision': rf_report['weighted avg']['precision'],\n",
        "    'Recall': rf_report['weighted avg']['recall'],\n",
        "    'F1-score': rf_report['weighted avg']['f1-score']\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_rf_scores = pd.DataFrame(rf_scores, index=['Random Forest'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Randomized search params\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [5, 10, 20, None]\n",
        "}\n",
        "random_search = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=5, cv=3, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = random_search.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "ConfusionMatrixDisplay.from_estimator(random_search, X_test, y_test)\n",
        "\n",
        "\n",
        "df_rf_scores.plot(kind='bar', figsize=(8, 5), legend=True, colormap='coolwarm')\n",
        "plt.title(\"Random Forest - Evaluation Metric Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(rf_random, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters (like learning rate, max depth, number of trees) significantly affect the performance and generalization of machine learning models. Tuning them properly ensures the model is neither underfitting nor overfitting and achieves maximum predictive power."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int(\"n_estimators\", 50, 200),\n",
        "        'max_depth': trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
        "    }\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **params)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    return accuracy_score(y_test, preds)\n",
        "\n",
        "# Run Optuna\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Train best model\n",
        "best_params = study.best_params\n",
        "print(\"Best Params from Optuna:\", best_params)\n",
        "\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **best_params)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "ConfusionMatrixDisplay.from_estimator(xgb_model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating the performance of all three models—Logistic Regression, Random Forest, and XGBoost—we selected XGBoost as the final prediction model for this project.\n",
        "Superior Predictive Performance:\n",
        "Among all the models, XGBoost consistently delivered the highest F1-score, precision, and recall, especially on the imbalanced dataset. Its gradient boosting approach helps in reducing both bias and variance, leading to better generalization.\n",
        "\n",
        "Handling Imbalanced Data:\n",
        "XGBoost supports built-in strategies like scale_pos_weight and max_delta_step to effectively handle class imbalance—an essential requirement in real-world PhonePe data like fraud detection or insurance claim anomalies.\n",
        "\n",
        "Robustness to Outliers and Noise:\n",
        "Unlike logistic regression, XGBoost is less sensitive to outliers and noisy features due to its tree-based structure."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) is a high-performance, tree-based ensemble machine learning algorithm that builds models in a sequential manner and focuses on minimizing errors at each stage. It works especially well on structured/tabular data, like PhonePe transaction data."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aimed to extract actionable insights and build predictive models using aggregated transaction data from the PhonePe platform. Through a structured pipeline involving data extraction, cleaning, transformation, visualization, model training, and explainability, we achieved meaningful results that hold high business value."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}